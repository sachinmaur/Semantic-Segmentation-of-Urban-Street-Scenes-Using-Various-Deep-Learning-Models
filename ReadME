# Semantic Segmentation of Urban Street Scenes Using Various Deep Learning Models

This project implements **semantic segmentation** of urban street scenes using three models: **SegFormer, SegFormer with Few-Shot Learning, and Segmenter**. These models leverage **transformer-based architectures** to classify each pixel in an image into predefined categories such as roads, vehicles, pedestrians, and buildings. 

We train and evaluate these models using the **CityScapes dataset**, a large-scale benchmark dataset designed for urban scene understanding.

## Dataset: CityScapes

The **CityScapes dataset** contains:

- High-resolution urban street images
- 19 semantic classes
- Annotated images for training and validation
- Available under a research-only license from the official CityScapes website.

## Models 

### 1. SegFormer with Few-Shot Learning
### 2. Segmenter (Vision Transformer-Based Model)
### 3. SegFormer
## Evaluation Metrics:
  - Intersection over Union (IoU)
  - Mean IoU (mIoU)
  - Pixel Accuracy
  - Mean Pixel Accuracy
## References

- Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., & Luo, P. (2021). "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers". [arXiv:2105.15203](https://arxiv.org/abs/2105.15203)
- Strudel, R., Garcia, R., Laptev, I., & Schmid, C. (2021). "Segmenter: Transformer for Semantic Segmentation". [arXiv:2105.05633](https://arxiv.org/abs/2105.05633)



