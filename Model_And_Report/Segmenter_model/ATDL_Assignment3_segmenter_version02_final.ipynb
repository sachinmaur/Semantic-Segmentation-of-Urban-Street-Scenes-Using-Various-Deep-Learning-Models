{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPf00fhA3oEYy6/FFsPNTEN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-N7ePQwBpc9b","executionInfo":{"status":"ok","timestamp":1739053557074,"user_tz":-330,"elapsed":23215,"user":{"displayName":"Sachin Maurya","userId":"06912416480278473161"}},"outputId":"7671a6a9-6a89-4dec-acc2-de40167eeedf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBj6JnnLZcdy","executionInfo":{"status":"ok","timestamp":1739053763529,"user_tz":-330,"elapsed":62271,"user":{"displayName":"Sachin Maurya","userId":"06912416480278473161"}},"outputId":"99b9cee0-c9c8-43f9-fab5-6e0cd7c0b80b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Train Loss: 3.1272, Val Loss: 1.6918\n","Epoch [2/5], Train Loss: 1.6668, Val Loss: 1.4343\n","Epoch [3/5], Train Loss: 1.2402, Val Loss: 1.3652\n","Epoch [4/5], Train Loss: 1.0212, Val Loss: 1.1234\n","Epoch [5/5], Train Loss: 0.7467, Val Loss: 0.9432\n","âœ… Loss plot saved at: /content/drive/MyDrive/A_deep_learning_lab/ATDL_Assignment03/Small_data_set/Out_put/training_output/loss_plot.png\n"]}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import timm\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import fnmatch\n","# ----------------------------\n","# 1. Define Paths and Directories\n","# ----------------------------\n","dataset_path = \"/content/drive/MyDrive/A_deep_learning_lab/ATDL_Assignment03/Small_data_set/data\"\n","output_path = \"/content/drive/MyDrive/A_deep_learning_lab/ATDL_Assignment03/Small_data_set/Out_put\"\n","train_image_path = os.path.join(dataset_path, \"train\", \"image\")\n","train_depth_path = os.path.join(dataset_path, \"train\", \"depth\")\n","train_label_path = os.path.join(dataset_path, \"train\", \"label\")\n","os.makedirs(output_path, exist_ok=True)\n","predictions_output_path = os.path.join(output_path, \"predictions\")\n","os.makedirs(predictions_output_path, exist_ok=True)\n","logs_path = os.path.join(output_path, \"logs.txt\")\n","data_visualization_path = os.path.join(output_path, \"data_visualization\")\n","os.makedirs(data_visualization_path, exist_ok=True)\n","\n","\n","training_output_path = os.path.join(output_path, \"training_output\")\n","log_path = os.path.join(training_output_path, \"training_logs.txt\")\n","loss_plot_path = os.path.join(training_output_path, \"loss_plot.png\")\n","model_save_path = os.path.join(training_output_path, \"trained_model.pth\")\n","evaluation_output_path = os.path.join(output_path, \"evaluation\")\n","\n","# Create directories if they do not exist\n","\n","os.makedirs(training_output_path, exist_ok=True)\n","os.makedirs(evaluation_output_path, exist_ok=True)\n","\n","\n","\n","\n","\n","# ----------------------------\n","# 2. Dataset Class\n","# ----------------------------\n","class CityScapes(Dataset):\n","    def __init__(self, root, train=True):\n","        self.train = train\n","        self.root = os.path.expanduser(root)\n","        self.data_path = os.path.join(root, 'train' if train else 'val')\n","        self.data_len = len([f for f in os.listdir(os.path.join(self.data_path, 'image')) if f.endswith('.npy')])\n","        self.transform_image = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images\n","        ])\n","\n","        self.transform_label = transforms.Compose([\n","            transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.NEAREST),\n","        ])\n","\n","    def __getitem__(self, index):\n","    # Load the data\n","        image = np.load(os.path.join(self.data_path, 'image', f'{index}.npy')).astype(np.float32)\n","        label = np.load(os.path.join(self.data_path, 'label', f'{index}.npy')).astype(np.int64)  # Load as int64\n","        depth = np.load(os.path.join(self.data_path, 'depth', f'{index}.npy')).astype(np.float32)\n","\n","        # Convert the image to uint8 for PIL compatibility\n","        image = (image * 255).astype(np.uint8)\n","\n","        # Ensure label is in int64 before modifying values\n","        label = label.astype(np.int64)\n","\n","        # Replace 255 (ignore regions) with -1 for CrossEntropyLoss\n","        label[label == 255] = -1  # Ensures ignore regions are properly marked\n","\n","        # Convert label to uint8 before using PIL.Image (PIL does not support int64)\n","        label = label.astype(np.uint8)\n","\n","        # Apply transformations\n","        image = self.transform_image(Image.fromarray(image))\n","\n","        # Resize label manually and preserve class values\n","        label = Image.fromarray(label)\n","        label = label.resize((224, 224), resample=Image.NEAREST)\n","        label = np.array(label).astype(np.int64)  # Convert back to int64 for PyTorch compatibility\n","\n","        # Ensure label values are within valid class range\n","        label[label > num_classes - 1] = -1  # Map out-of-range values to -1\n","\n","        # Convert label to PyTorch tensor\n","        label = torch.tensor(label, dtype=torch.long)\n","\n","        return {\n","            'image': image,\n","            'semantic': label,  # Properly mapped to valid class range\n","            'depth': torch.tensor(depth, dtype=torch.float32)  # Depth remains float32\n","        }\n","\n","\n","    def __len__(self):\n","        return self.data_len\n","\n","\n","\n","# ----------------------------\n","# 3. DataLoader\n","# ----------------------------\n","train_dataset = CityScapes(dataset_path, train=True)\n","val_dataset = CityScapes(dataset_path, train=False)\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n","\n","# ----------------------------\n","# 4. Segmenter Model Definition\n","# ----------------------------\n","class Segmenter(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Segmenter, self).__init__()\n","        self.encoder = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(768, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, num_classes, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        features = self.encoder.forward_features(x)\n","        features = features[:, 1:, :].permute(0, 2, 1).reshape(x.size(0), 768, 14, 14)\n","        segmentation_mask = self.decoder(features)\n","        return nn.functional.interpolate(segmentation_mask, scale_factor=16, mode='bilinear', align_corners=False)\n","\n","num_classes = 20\n","model = Segmenter(num_classes)\n","\n","# ----------------------------\n","# 5. Loss and Optimizer\n","# ----------------------------\n","criterion = nn.CrossEntropyLoss(ignore_index=-1)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Reduce learning rate\n","\n","\n","# ----------------------------\n","# 6. Visualization Functions\n","# ----------------------------\n","def visualize_sample(image_path, depth_path, label_path, index, output_dir):\n","    img_files = sorted(os.listdir(image_path))\n","    depth_files = sorted(os.listdir(depth_path))\n","    label_files = sorted(os.listdir(label_path))\n","\n","    index = min(index, len(img_files) - 1, len(depth_files) - 1, len(label_files) - 1)\n","\n","    img = np.load(os.path.join(image_path, img_files[index]))\n","    depth = np.load(os.path.join(depth_path, depth_files[index])).squeeze()\n","    label = np.load(os.path.join(label_path, label_files[index])).squeeze()\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","    axes[0].imshow(img)\n","    axes[0].set_title(\"RGB Image\")\n","\n","    axes[1].imshow(depth, cmap='gray')\n","    axes[1].set_title(\"Depth Map\")\n","\n","    axes[2].imshow(label, cmap='jet')\n","    axes[2].set_title(\"Segmentation Label\")\n","\n","    save_path = os.path.join(output_dir, f\"sample_visualization_{index}.png\")\n","    plt.savefig(save_path)\n","    plt.close()\n","    return save_path\n","\n","# Visualize a few samples from train and val datasets\n","with open(os.path.join(data_visualization_path, \"data_visualization_log.txt\"), \"w\") as log_file:\n","    for i in range(3):\n","        train_sample_path = visualize_sample(train_image_path, train_depth_path, train_label_path, i, data_visualization_path)\n","        # val_sample_path = visualize_sample(val_image_path, val_depth_path, val_label_path, i, data_visualization_path)\n","        log_file.write(f\"Train Sample {i} saved at: {train_sample_path}\\n\")\n","        # log_file.write(f\"Val Sample {i} saved at: {val_sample_path}\\n\")\n","\n","\n","\n","# ----------------------------\n","# 7. Training Function\n","# ----------------------------\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, device='cuda'):\n","    model.to(device)\n","    train_losses, val_losses = [], []\n","    with open(logs_path, \"w\") as log_file:\n","        for epoch in range(num_epochs):\n","            model.train()\n","            running_loss = 0.0\n","            for batch in train_loader:\n","                images = batch['image'].to(device)\n","                labels = batch['semantic'].to(device)\n","                optimizer.zero_grad()\n","                outputs = model(images)\n","                labels = nn.functional.interpolate(labels.unsqueeze(1).float(), size=(224, 224), mode='nearest').squeeze(1).long()\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item()\n","\n","            val_loss = 0.0\n","            model.eval()\n","            with torch.no_grad():\n","                for batch in val_loader: # Iterate through the validation dataloader (which returns dictionaries)\n","                    images = batch['image'].to(device) # Access the 'image' tensor from the dictionary\n","                    labels = batch['semantic'].to(device) # Access the 'semantic' tensor (labels) from the dictionary\n","                    outputs = model(images)\n","                    loss = criterion(outputs, labels.long()) # Calculate the loss\n","                    val_loss += loss.item() # Accumulate the validation loss\n","\n","            train_losses.append(running_loss / len(train_loader))\n","            val_losses.append(val_loss / len(val_loader))\n","            log_file.write(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\\n\")\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n","\n","    # Save loss curve\n","     # âœ… Save loss curve\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n","    plt.plot(range(1, num_epochs + 1), val_losses, label='Val Loss', marker='s')\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training & Validation Loss Curve\")\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.savefig(loss_plot_path)  # âœ… Save the plot\n","    plt.close()\n","\n","    print(f\"âœ… Loss plot saved at: {loss_plot_path}\")\n","\n","    torch.save(model.state_dict(), model_save_path)\n","# ----------------------------\n","# 8. Visualization Predictions\n","# ----------------------------\n","def visualize_predictions(model, val_loader, device='cuda'):\n","    model.to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        for idx, batch in enumerate(val_loader):\n","            images = batch['image'].to(device)\n","            depths = batch['depth']\n","            labels = batch['semantic']\n","            outputs = model(images)\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","\n","            for i in range(len(images)):\n","                image = images[i].permute(1, 2, 0).cpu().numpy() * 255\n","                depth = depths[i].numpy()\n","                label = labels[i].numpy()\n","                pred = preds[i]\n","\n","                fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n","                axes[0].imshow(image.astype(np.uint8))\n","                axes[0].set_title(\"RGB Image\")\n","                axes[1].imshow(depth, cmap='gray')\n","                axes[1].set_title(\"Depth Map\")\n","                axes[2].imshow(label, cmap='jet')\n","                axes[2].set_title(\"Ground Truth\")\n","                axes[3].imshow(pred, cmap='jet')\n","                axes[3].set_title(\"Predicted Segmentation\")\n","\n","                save_path = os.path.join(predictions_output_path, f\"visualization_{idx}_{i}.png\")\n","                plt.savefig(save_path)\n","                plt.close()\n","\n","# ----------------------------\n","# 9. Run Training and Visualization\n","# ----------------------------\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, device=device)\n","visualize_predictions(model, val_loader, device=device)\n"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","def evaluate_model(model, dataloader, num_classes, device='cuda'):\n","    model.to(device)\n","    model.eval()\n","\n","    intersection = np.zeros(num_classes)  # True Positives (TP)\n","    union = np.zeros(num_classes)         # TP + FP + FN\n","    pixel_acc_total = 0\n","    pixel_acc_count = 0\n","    mean_acc_total = np.zeros(num_classes)\n","    mean_acc_count = np.zeros(num_classes)\n","\n","    with torch.no_grad():\n","        for batch in dataloader:  # FIXED: Extract data correctly\n","            images = batch['image'].to(device)\n","            labels = batch['semantic'].to(device)  # Semantic segmentation labels\n","\n","            # Ensure label dimensions are correct\n","            if labels.dim() == 3:  # Expecting [batch, H, W]\n","                labels = labels.unsqueeze(1)  # Add channel dimension -> [batch, 1, H, W]\n","\n","            labels = torch.nn.functional.interpolate(labels.float(), size=(224, 224), mode='nearest').squeeze(1).to(torch.long)\n","\n","            # Get predictions\n","            outputs = model(images)\n","            preds = torch.argmax(outputs, dim=1)  # Convert logits to class predictions\n","\n","            for i in range(num_classes):\n","                pred_mask = preds == i\n","                true_mask = labels == i\n","\n","                intersection[i] += torch.sum(pred_mask & true_mask).item()  # True Positives\n","                union[i] += torch.sum(pred_mask | true_mask).item()         # True Positives + False Positives + False Negatives\n","\n","                mean_acc_total[i] += torch.sum(true_mask & pred_mask).item()\n","                mean_acc_count[i] += torch.sum(true_mask).item()\n","\n","            # Compute pixel accuracy\n","            correct_pixels = (preds == labels).sum().item()\n","            total_pixels = labels.numel()\n","\n","            pixel_acc_total += correct_pixels\n","            pixel_acc_count += total_pixels\n","\n","    # Compute IoU per class\n","    iou_per_class = intersection / (union + 1e-8)  # Avoid division by zero\n","    mean_iou = np.mean(iou_per_class)  # Compute mean IoU\n","\n","    # Compute mean pixel accuracy per class\n","    mean_pixel_acc_per_class = mean_acc_total / (mean_acc_count + 1e-8)\n","\n","    # Compute overall pixel accuracy\n","    pixel_accuracy = pixel_acc_total / pixel_acc_count\n","\n","    # Logging results\n","\n","# Logging results\n","    eval_file_path = os.path.join(output_path, \"evaluation_results.txt\")\n","\n","    with open(eval_file_path, \"a\") as log_file:  # âœ… Fix: Append mode\n","        log_file.write(\"\\n===== Evaluation Results =====\\n\")\n","        log_file.write(f\"Pixel Accuracy: {pixel_accuracy:.4f}\\n\")\n","        log_file.write(f\"Mean Pixel Accuracy: {np.mean(mean_pixel_acc_per_class):.4f}\\n\")\n","        log_file.write(f\"Mean IoU (mIoU): {mean_iou:.4f}\\n\")\n","        log_file.write(f\"IoU per class:\\n\")\n","        for i, iou in enumerate(iou_per_class):\n","            log_file.write(f\"Class {i}: IoU {iou:.4f}\\n\")\n","\n","    # Debugging: Check if results are written correctly\n","    print(f\"âœ… Evaluation results saved at: {eval_file_path}\")\n","\n","    print(f\"âœ… Pixel Accuracy: {pixel_accuracy:.4f}\")\n","    print(f\"âœ… Mean Pixel Accuracy: {np.mean(mean_pixel_acc_per_class):.4f}\")\n","    print(f\"âœ… Mean IoU (mIoU): {mean_iou:.4f}\")\n","    print(\"âœ… IoU per class:\", iou_per_class)\n","\n","    return pixel_accuracy, np.mean(mean_pixel_acc_per_class), mean_iou, iou_per_class\n"],"metadata":{"id":"aw51670RorVs","executionInfo":{"status":"ok","timestamp":1739053763659,"user_tz":-330,"elapsed":98,"user":{"displayName":"Sachin Maurya","userId":"06912416480278473161"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["evaluate_model(model, val_loader, num_classes=20, device='cuda' if torch.cuda.is_available() else 'cpu')\n","\n"],"metadata":{"id":"GNorSgGMo1Dq","executionInfo":{"status":"ok","timestamp":1739053767843,"user_tz":-330,"elapsed":4174,"user":{"displayName":"Sachin Maurya","userId":"06912416480278473161"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"99b2e28e-3d7d-451c-ebc2-3063c8b1ca4b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Evaluation results saved at: /content/drive/MyDrive/A_deep_learning_lab/ATDL_Assignment03/Small_data_set/Out_put/evaluation_results.txt\n","âœ… Pixel Accuracy: 0.6607\n","âœ… Mean Pixel Accuracy: 0.1321\n","âœ… Mean IoU (mIoU): 0.0877\n","âœ… IoU per class: [6.36525760e-01 0.00000000e+00 6.65397470e-01 0.00000000e+00\n"," 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n"," 4.52779936e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n"," 0.00000000e+00 6.11770464e-05 0.00000000e+00 0.00000000e+00\n"," 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.6606744260204082,\n"," 0.13211289904122536,\n"," 0.08773821711626545,\n"," array([6.36525760e-01, 0.00000000e+00, 6.65397470e-01, 0.00000000e+00,\n","        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","        4.52779936e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","        0.00000000e+00, 6.11770464e-05, 0.00000000e+00, 0.00000000e+00,\n","        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"1kvwiIEro2Fp"},"execution_count":null,"outputs":[]}]}